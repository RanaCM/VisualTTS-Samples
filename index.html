<!doctype html>
<html lang="en">

<head>
  <meta name="google-site-verification" content="ftFOlJETX-2KNjaPh8W6s8lhigItRuu9fOmjHZZ0nY0" />
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"
    integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

  <title>VisualTTS</title>
</head>
<style type="text/css">
  table {
    width: 100%;
    table-layout: fixed;
  }

  audio {
    width: 100%;
  }

  thead>tr>th:first-child {
    width: 96px;
  }

  @media (max-width: 767px) {
    .big-screen {
      display: none;
    }
  }

  @media (min-width: 767px) {
    .small-screen {
      display: none;
    }
  }
</style>




<body>
  <header class="header">
    <div class="jumbotron bg-secondary text-center">
      <div class="container">
        <div class="row align-items-center">
          <div class="col-md-12">
 
            <h1><a class="text-light">VisualTTS: Text-to-Speech Synthesis with Visual Information <br> For Automatic Voice Over</h1><br> 
             <font size=5><span style="color:#FFFFFF"> Authors: Junchen Lu, Berrak Sisman, Rui Liu, Mingyang Zhang, Haizhou Li <br></font>
			 <!--<font size=5><span style="color:#FFFFFF"> This page is currently under construction. <br> Speech samples will be updated soon. </font>	--> 
                </a>  
            <p>
              <div class="row">
              </div>
            </p>
            
          </div>
        </div>
      </div>
    </div>
  </header>
  <main>
    <div class="container">
      <div class="row" id="result">
        <div class="col-md-12">
        	<h5>Abstract:</h5> 
			In this paper, we formulate a novel task to synthesize speech in sync with a silent pre-recorded video, denoted as automatic voice over (AVO). Unlike traditional speech synthesis, AVO seeks to generate not only human-sounding speech, but also perfect lip-speech synchronization. A natural solution to AVO is to condition the speech rendering on the temporal progression of lip sequence in the video. We propose a novel text-to-speech model that is conditioned on visual input, named <i>VisualTTS</i>, for accurate lip-speech synchronization. The proposed VisualTTS adopts two novel mechanisms that are 1) textual-visual attention, and 2) visual fusion strategy during acoustic decoding, which both contribute to forming accurate alignment between the input text content and lip motion in input lip sequence. Experimental results show that VisualTTS achieves accurate lip-speech synchronization and outperforms all baseline systems.<br>
<br>
<br><br><br>
          <div style="text-align: center; width: 1100px;">
          	<img align="center" src="workflow.png"  width="600" height="200"  />
 				<br>
        <br>
                    Fig. 1: The typical workflow of automatic voice over: <br> An AVO framework takes lip image sequence and text script as input, and generates speech audio in sync with video. <br>
                    
          </div>
		<br><br><br>
           <div style="text-align: center; width: 1100px;">
            <img align="center" src="model.png"  width="600" height="520"  />
        <br>
        <br>
					Fig. 2: Model architecture of the proposed VisualTTS, that consists of visual encoder, textual encoder, <br>visual-guided aligner and acoustic decoder. Pre-trained blocks are denoted with a lock.
                    
          </div>


			<br><br><br>

      <h5>Speech Samples:</h5>
      We use a modified version of Tacotron [1] that takes no visual input as the Baseline. 
      We replace the original pre-recorded speeches in videos from the GRID dataset with synthetic speech samples produced by Baseline and VisualTTS. 
      Compared to Baseline, our VisualTTS achieves better lip-speech synchronization that is presented in videos below. 
      With the help of visual information, VisualTTS has better duration modeling performance. The duration distortion between speech samples of VisualTTS and Ground Truth is smaller.
      VisualTTS can generate silent clips and pauses between phonemes where the input lip sequence indicates silence or little lip motion. 
      <br><br>
      <table align="center"><tr>
        <td align="center" width="90">Groud Truth</td>
        <td align="center"><video width="270" height="216" controls><source src=samples/target/s2_pbbp2s.mp4 type="video/mp4"></video></td>
        <td align="center"><img src=samples/target/s2_pbbp2s.png width="500"></td>
      </tr></table>

      <table align="center"><tr>
        <td align="center" width="90">Baseline</td>
        <td align="center"><video width="270" height="216" controls><source src=samples/baseline/s2_pbbp2s.mp4 type="video/mp4"></video></td>
        <td align="center"><img src=samples/baseline/s2_pbbp2s.png width="500"></td>
      </tr></table>

      <table align="center"><tr>
        <td align="center" width="90">VisualTTS</td>
        <td align="center"><video width="270" height="216" controls><source src=samples/proposed/s2_pbbp2s.mp4 type="video/mp4"></video></td>
        <td align="center"><img src=samples/proposed/s2_pbbp2s.png width="500"></td>
      </tr></table>
      

      <br>
      <table align="center"><tr>
        <td align="center" width="90">Groud Truth</td>
        <td align="center"><video width="270" height="216" controls><source src=samples/target/s10_pwatzn.mp4 type="video/mp4"></video></td>
        <td align="center"><img src=samples/target/s10_pwatzn.png width="500"></td>
      </tr></table>

      <table align="center"><tr>
        <td align="center" width="90">Baseline</td>
        <td align="center"><video width="270" height="216" controls><source src=samples/baseline/s10_pwatzn.mp4 type="video/mp4"></video></td>
        <td align="center"><img src=samples/baseline/s10_pwatzn.png width="500"></td>
      </tr></table>

      <table align="center"><tr>
        <td align="center" width="90">VisualTTS</td>
        <td align="center"><video width="270" height="216" controls><source src=samples/proposed/s10_pwatzn.mp4 type="video/mp4"></video></td>
        <td align="center"><img src=samples/proposed/s10_pwatzn.png width="500"></td>
      </tr></table>


      <br>
      <table align="center"><tr>
        <td align="center" width="90">Groud Truth</td>
        <td align="center"><video width="270" height="216" controls><source src=samples/target/s34_brwq1n.mp4 type="video/mp4"></video></td>
        <td align="center"><img src=samples/target/s34_brwq1n.png width="500"></td>
      </tr></table>

      <table align="center"><tr>
        <td align="center" width="90">Baseline</td>
        <td align="center"><video width="270" height="216" controls><source src=samples/baseline/s34_brwq1n.mp4 type="video/mp4"></video></td>
        <td align="center"><img src=samples/baseline/s34_brwq1n.png width="500"></td>
      </tr></table>

      <table align="center"><tr>
        <td align="center" width="90">VisualTTS</td>
        <td align="center"><video width="270" height="216" controls><source src=samples/proposed/s34_brwq1n.mp4 type="video/mp4"></video></td>
        <td align="center"><img src=samples/proposed/s34_brwq1n.png width="500"></td>
      </tr></table>


      <br>
      <table align="center"><tr>
        <td align="center" width="90">Groud Truth</td>
        <td align="center"><video width="270" height="216" controls><source src=samples/target/s24_sbik7p.mp4 type="video/mp4"></video></td>
        <td align="center"><img src=samples/target/s24_sbik7p.png width="500"></td>
      </tr></table>

      <table align="center"><tr>
        <td align="center" width="90">Baseline</td>
        <td align="center"><video width="270" height="216" controls><source src=samples/baseline/s24_sbik7p.mp4 type="video/mp4"></video></td>
        <td align="center"><img src=samples/baseline/s24_sbik7p.png width="500"></td>
      </tr></table>

      <table align="center"><tr>
        <td align="center" width="90">VisualTTS</td>
        <td align="center"><video width="270" height="216" controls><source src=samples/proposed/s24_sbik7p.mp4 type="video/mp4"></video></td>
        <td align="center"><img src=samples/proposed/s24_sbik7p.png width="500"></td>
      </tr></table>


      <br>
                     
                   

        <div class="col">
          <h5>References</h5>
    
          [1] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al., “Tacotron:  Towards end-to-end  speech synthesis, ”arXiv preprint arXiv:1703.10135,2017.
          <br>
        
        
        </div>
      </div>
      <!--<hr>
      <div class="row" id="contact">
          <div class="col">
            <h2>Contact</h2>
            <div></div>
          </div>
        </div> -->
<!--<div align="center" style="margin:auto;padding-top:10px">
        <div style="width:100%">
        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=YvIW2uh8CTeJY0rhYoMoKE6zPfPKHthroRgvJRTSdjg&cl=ffffff&w=a"></script>
        </div>
</div>-->
<br><br>

    </div>
  </main>
  <footer class="bg-secondary text-light mt-4 pt-3 pb-2 ">
    <div class="container">
      <p class="text-center">
        
      </p>
    </div>
  </footer>
  <!-- Optional JavaScript -->
  <!-- jQuery first, then Popper.js, then Bootstrap JS -->
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
</body>

</html>
